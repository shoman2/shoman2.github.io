---
layout: post
title:  "이미지처리 - Yolo v1"
subtitle:   "Object Detection"
categories: data
tags: dl
comments: true
---
# YOLO V1 - 개요

### You Only Look Once: Unified, Real-Time Object Detection

First, YOLO is extremely fast

Unlike sliding window and region proposal-based techniques, **YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance.** Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. **YOLO makes less than half the number of background errors compared to Fast R-CNN**

Third, YOLO learns **generalizable representations of objects.** When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is **less likely to break down** **when applied to new domains or unexpected inputs.**





# YOLO V1 - Network (paper 기준)

Input: Image

Process:

Output:  7 × 7 × 30 tensor of predictions

S = 7 (7 x7 그리드셀을 긋는다.)

B = 2 (그리드 셀마다 바운딩박스 2개친다)

C = 20 (20개 라벨을 구분해야한다)



**"Unified Detection"**

Hyperparameter:

- S ; grid
- B;bbox
- C;class

아웃풋사이즈는?

7 x 7 x 30 = SxS **곱하기** (바운딩박스 수 =**2**  **곱하기 (**xywh 4 + conf score 1 =총 **5**) **더하기** 라벨수 **20**)

<deepsystems.io의 네트워크에 대한 상세설명 참조하기>



# YOLO v1 - LOSS



### Loss 를 5개로 쪼개면..

(1) Object가 **존재하는** 그리드 셀 i의 bounding box predictor j에 대해, **x와 y의 loss**를 계산.

(2) Object가 **존재하는** 그리드 셀 i의 bounding box predictor j에 대해, **w와 h의 loss**를 계산. 큰 box에 대해서는 작은 분산(small deviation)을 반영하기 위해 제곱근을 취한 후, sum-squared error를 구합니다. (같은 error라도 큰 box의 경우 상대적으로 IOU에 영향을 적게 줍니다.)

(3) Object가 **존재하는** 그리드 셀 i의 bounding box predictor j에 대해, **confidence score**의 loss를 계산. (Ci = 1)

(4) Object가 존재하지 않는 그리드 셀 i의 bounding box predictor j에 대해, confidence score의 loss를 계산. (Ci = 0)

(5) Object가 **존재하는** 그리드 셀 i에 대해, conditional **class probability**의 loss를 계산. (p_i(c)=1 if class c is correct, otherwise: p_i(c)=0)



### Yolo Training?

YOLO는 총 24개의 컨볼루션 계층(convolutional layers)과 2개의 전결합 계층(fully connected layers)으로 구성

- ImageNet 1000-class dataset으로 20개의 convolutioanl layer를 pre-training

- Pre-training 이후 **4 convolutioanl layers와 2 fully connected layers를 추가**

  - **20개 프리트레인 모델(Modified GoogLeNet) + 4conv layer + 2 fc layer**

- Bounding Box의 width와 height는 이미지의 width와 height로 **nomalize** (Range: 0~1)

- Bounding Box의 x와 y는 특정 grid cell 위치의 offset값을 사용한다 (Range: 0~1)

- λcoord: 5, : 0.5

  λcoord

  λnoobj

  λnoobj

- Batch size: 64

- Momentum: 0.9 and a decay of 0.0005

- Learning Rate: 0.001에서 0.01로 epoch마다 천천히 상승시킴. 이후 75 epoch동안 0.01, 30 epoch동안 0.001, 마지막 30 epoch동안 0.0001

- Dropout Rate: 0.5

- Data augmentation: random scailing and translations of up to 20% of the original image size

- Activation function: **leaky rectified linear activation**



### **욜로의 Constraints ?**

하나의 그리드 셀은 오직 하나의 객체만 검출하므로 하나의 그리드 셀에 두 개 이상의 객체가 붙어있다면 이를 잘 검출하지 못하는 떼와 같이 작은 물체가 몰려 있는 경우 공간적 제약 때문에 객체 검출이 제한적일 수밖에...

하나의 그리드 셀은 오직 하나의 객체만 검출하는데 여러 객체가 몰려있으면 검출하지 못하는 객체도 존재

YOLO 모델은 데이터로부터 bounding box를 예측하는 것을 학습하기 때문에 훈련 단계에서 학습하지 못했던 새로운 종횡비(aspect ratio, 가로 세로 비율)를 마주하면 고전할 수밖에

YOLO 모델은 큰 bounding box와 작은 bounding box의 loss에 대해 동일한 가중치를 둔다는 단점
